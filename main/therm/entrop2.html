<HTML>
<!-- Mirrored from hyperphysics.phy-astr.gsu.edu/hbase/therm/entrop2.html by HTTrack Website Copier/3.x [XR&CO'2010], Tue, 27 Dec 2011 05:30:36 GMT -->
<HEAD>   <TITLE>Entropy</TITLE></HEAD><body bgcolor="#FFE4C8"><!-- Copyright 2001 by Carl Rod Nave, RodNave@gsu.edu--><A Name="c1"></A><TABLE WIDTH="580" HEIGHT="370" BORDER="1" CELLSPACING="2" CELLPADDING="2"><TR><TD WIDTH="514" HEIGHT="309"><H1 align=center>Entropy as a Measure of the Multiplicity of a System</H1><p>The probability of finding a system in a given state depends upon the multiplicity of that state. That is to say, it is proportional to the number of ways you can produce that state. Here a "state" is defined by some measurable property which would allow you to distinguish it from other states. In throwing a pair of dice, that measurable property is the sum of the number of dots facing up. The multiplicity for two dots showing is just one, because there is only one arrangement of the dice which will give that state.  The multiplicity for seven dots showing is six, because there are six arrangements of the dice which will show a total of seven dots.</p><center><img src="imgthe/dicemult.gif"></center><p>One way to define the quantity "entropy" is to do it in terms of the multiplicity. </p><center><font size="+3">Multiplicity = <font face="symbol">W</font></font></center><center><font size="+3">Entropy = k ln<font face="symbol">W</font></font></center><br><p>where k is Boltzmann's constant. The k is included as part of the  historical definition of entropy and gives the units Joule/Kelvin in the SI system of units. The logarithm is used to make the defined entropy of reasonable size. The multiplicity for ordinary collections of matter are on the order of Avogadro's number, so using the logarithm of the multiplicity is convenient.</p><p>For a system of a large number of particles, like a mole of atoms, the most probable state will be overwhelmingly probable. You can with confidence expect that the system at equilibrium will be found in the state of highest multiplicity since fluctuations from that state will usually be too small to measure. As a large system approaches equilibrium, its multiplicity (entropy) tends to increase. This is a way of stating the <a href="../thermo/seclaw.html#c1">second law of thermodynamics</a>.</p><center><table BORDER="1" CELLSPACING="2" CELLPADDING="2"><tr><td><center><a href="entrop.html#e3">Entropy and disorder</a></center></td></tr></table></center></TD><TD WIDTH="66" align=center><a href="../hframe.html">Index</a><br><br><a href="entropcon.html#c1">Entropy concepts</a><br><br>Reference<br><a href="../thermo/heatrf.html#c1">Schroeder</a><br>Ch 2</TD></TR><TR><TD HEIGHT="17">&nbsp;<table><tr><td width= "450"><A HREF="../hph.html"> HyperPhysics</A>*****<A HREF="../heacon.html#heacon">     Thermodynamics </A></td><td align=right><font size="-1"><i>R Nave</i></font></td></tr></table> </A></TD><TD><a href="Javascript:history.go(-1)">Go Back</a></TD></TR></TABLE><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><A Name="c2"></A><TABLE WIDTH="580" HEIGHT="370" BORDER="1" CELLSPACING="2" CELLPADDING="2"><TR><TD WIDTH="514" HEIGHT="309"><H1 align=center>Entropy in Terms of Heat and Temperature</H1><p>The relationship which was originally used to define entropy S is</p><center><font size="+3">dS = dQ/T</font></center><p>This is often a sufficient definition of entropy if you don't need to know about the microscopic details. It can be integrated to calculate the change in entropy during a part of an engine cycle. For the case of an <a href="../thermo/isoth.html#c1">isothermal process</a> it can be evaluated simply by &#916S = Q/T.</p><p>In this context, a the change in entropy can be described as the <a href="../thermo/heat.html#c1">heat</a> added per unit <a href="../thermo/temper.html#c1">temperature</a> and has the units of Joules/Kelvin (J/K) or eV/K.</p><center><table BORDER="1" CELLSPACING="2" CELLPADDING="2"><tr><td><center><a href="entrop.html#e3">Entropy and disorder</a></center></td></tr></table></center></TD><TD WIDTH="66" align=center><a href="../hframe.html">Index</a><br><br><a href="entropcon.html#c1">Entropy concepts</a><br><br>Reference<br><a href="../thermo/heatrf.html#c1">Schroeder</a><br>Ch 3</TD></TR><TR><TD HEIGHT="17">&nbsp;<table><tr><td width= "450"><A HREF="../hph.html"> HyperPhysics</A>*****<A HREF="../heacon.html#heacon">     Thermodynamics </A></td><td align=right><font size="-1"><i>R Nave</i></font></td></tr></table> </A></TD><TD><a href="Javascript:history.go(-1)">Go Back</a></TD></TR></TABLE><BR><BR><BR><BR><BR></BODY>
<!-- Mirrored from hyperphysics.phy-astr.gsu.edu/hbase/therm/entrop2.html by HTTrack Website Copier/3.x [XR&CO'2010], Tue, 27 Dec 2011 05:30:37 GMT -->
</HTML>